{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshDaga/ML_Assignment/blob/master/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekXdTtltP0bl",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px-naBATP3CP",
        "colab_type": "text"
      },
      "source": [
        "Sol 1. OLS method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyLmyOkP1lc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "971ddb40-5072-4ed4-98f8-9e01999e898d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "data = pd.DataFrame(pd.read_excel(\"/content/drive/My Drive/slr06.xls\"))\n",
        "X = data['X'].values\n",
        "y = data['Y'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "mean_x = np.mean(X_train)\n",
        "mean_y = np.mean(y_train)\n",
        "covmat = np.cov(X_train,y_train)\n",
        "m= covmat[0][1]/covmat[0][0]\n",
        "c = mean_y - (m * mean_x)\n",
        "print(m,c)\n",
        "y_train_res = (m*X_train)+c\n",
        "y_test_res = (m*X_test)+c\n",
        "print(\"mean absolute error on train set is: \",mean_absolute_error(y_train,y_train_res))\n",
        "print(\"mean absolute error on test set is: \",mean_absolute_error(y_test,y_test_res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
            "3.404887445482695 22.296085203742095\n",
            "mean absolute error on train set is:  29.496246635464164\n",
            "mean absolute error on test set is:  24.838142991133815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzSUNN1kWPGo",
        "colab_type": "text"
      },
      "source": [
        "Sol 2. Gradient Descent Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEPCmzhpUYoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "f149406a-b96b-4f43-82d5-5c88e1b6aa2d"
      },
      "source": [
        "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.25,random_state=1)\n",
        "theta_0,theta_1,alpha =0,0,0.0001\n",
        "prev_theta_0 = prev_theta_1 = -1\n",
        "n = float(len(X_train))\n",
        "while (prev_theta_0!=theta_0 and prev_theta_1!=theta_1):\n",
        "  y_predicted_train = theta_0 + (theta_1*X_train)\n",
        "  prev_theta_0,prev_theta_1 = theta_0,theta_1\n",
        "  cf_theta_0 = (1/n)*(np.sum(y_predicted_train - y_train))\n",
        "  cf_theta_1 = (1/n)*(np.sum(X_train*(y_predicted_train-y_train)))\n",
        "  theta_0 = theta_0 - (alpha*cf_theta_0)\n",
        "  theta_1 = theta_1 - (alpha*cf_theta_1)\n",
        "print(theta_0,theta_1)\n",
        "y_predicted_train = theta_0 + (theta_1*X_train)\n",
        "y_predicted_test = theta_0 + (theta_1*X_test)\n",
        "y_predicted_val =  theta_0 + (theta_1*X_val)\n",
        "print('MAE in training set:',mean_absolute_error(y_train,y_predicted_train))\n",
        "print('MAE in validation set:',mean_absolute_error(y_val,y_predicted_val))\n",
        "print('MAE in testing set: ',mean_absolute_error(y_test,y_predicted_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19.883112423697547 3.4230586885087346\n",
            "MAE in training set: 28.995699850333967\n",
            "MAE in validation set: 31.453019377642015\n",
            "MAE in testing set:  24.028888510902316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb8NhKqpQAth",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn_BKjIiQBSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "37e17f84-dd03-4337-dd6b-49890f0f9c2b"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston_dataset = load_boston(return_X_y=False)\n",
        "#print(boston_dataset.keys())\n",
        "#print(boston_dataset.DESCR)\n",
        "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "boston['MEDV'] = boston_dataset.target # Median value of owner-occupied homes in $1000s ( we have to predict it )\n",
        "#boston.head(10)\n",
        "data = boston.to_numpy() #converting DataFrames to numpy array\n",
        "print(\"boston data shape (row,column) : \",data.shape)\n",
        "\n",
        "#dividing data into training set , validation set and test set\n",
        "sz = data[:,0].size\n",
        "train_data = data[0:int(sz*0.6),:]\n",
        "validate_data = data[int(sz*0.6) : int(sz*0.8)   , :]\n",
        "test_data = data[int(sz*0.8)  : , :]\n",
        "print(\"total_data : \" ,sz)\n",
        "print(\"train_data : \" , train_data[:,0].size)\n",
        "print(\"validation_data : \" ,validate_data[:,0].size)\n",
        "print(\"test_data : \" ,test_data[:,0].size) \n",
        "\n",
        "#======================================================================\n",
        "\n",
        "# gradientDescent function to calculate optimal theta\n",
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    m = y.size  # number of training examples\n",
        "    for i in range(num_iters):\n",
        "        y_hat = np.dot(X, theta) # y_predicted (predicted housing price)\n",
        "        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-y) \n",
        "    return theta\n",
        "\n",
        "\n",
        "# adding column of ones to the data sets for theta0 ( i.e. the first parameter int theta )\n",
        "n=train_data[:,0].size\n",
        "ones=np.ones((n,1))\n",
        "train_data = np.column_stack((ones,train_data)) # or like -> np.c_[ones, train_data]\n",
        "\n",
        "n=validate_data[:,0].size\n",
        "ones=np.ones((n,1))\n",
        "validate_data = np.column_stack((ones,validate_data))\n",
        "\n",
        "n=test_data[:,0].size\n",
        "ones=np.ones((n,1))\n",
        "test_data = np.column_stack((ones,test_data))\n",
        "\n",
        "\n",
        "\n",
        "print(\"=========== GRADIENT DESCENT ALGORITHM STARTS : ==============\")\n",
        "nrow,ncol = data.shape\n",
        "x = train_data[:,0:ncol]\n",
        "y = train_data[:,ncol:]\n",
        "theta = np.zeros((ncol,1))\n",
        "alpha = 0.000001\n",
        "iter = 100000\n",
        "theta = gradientDescent(x,y,theta,alpha,iter)\n",
        "print(\"theta from training set : \",theta)\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = MAE(y_predicted , y)\n",
        "\n",
        "# tuning the hyper-parameter (alpha) using validation data_set\n",
        "values_of_alpha = [0.000003 , 0.000001, 0.0000003 , 0.00000001 , 0.00000003]\n",
        "min_mae = mae\n",
        "res_alpha = alpha\n",
        "res_theta = theta\n",
        "x = validate_data[:,0:ncol]\n",
        "y = validate_data[:,ncol:]\n",
        "\n",
        "for alpha in values_of_alpha: # finding the best value of alpha which gives minimum value of MAE on validation set\n",
        "  theta = np.zeros((ncol,1))\n",
        "  theta = gradientDescent(x,y,theta,alpha,iter)\n",
        "  y_predicted = np.dot(x,theta) \n",
        "  mae = MAE(y_predicted , y)\n",
        "  if mae < min_mae:\n",
        "    res_alpha = alpha\n",
        "    res_theta = theta\n",
        "    min_mae = mae\n",
        "\n",
        "theta = res_theta\n",
        "print(\"theta after tuning alpha from validation set : \",theta)\n",
        " \n",
        "\n",
        "# calculating accuracy on train data \n",
        "x = train_data[:,0:ncol]\n",
        "y = train_data[:,ncol:]\n",
        "\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = MAE(y_predicted , y)\n",
        "print(\"MEAN ABSOLUTE ERROR ON TRAINING DATA : \",mae)\n",
        "\n",
        "\n",
        "# calculating accuracy on validation data \n",
        "x = validate_data[:,0:ncol]\n",
        "y = validate_data[:,ncol:]\n",
        "\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = MAE(y_predicted , y)\n",
        "print(\"MEAN ABSOLUTE ERROR ON VALIDATING DATA : \",mae)\n",
        "\n",
        "\n",
        "# calculating accuracy on test data \n",
        "x = test_data[:,0:ncol]\n",
        "y = test_data[:,ncol:]\n",
        "\n",
        "y_predicted = np.dot(x,theta) \n",
        "mae = MAE(y_predicted , y)\n",
        "print(\"MEAN ABSOLUTE ERROR ON TEST  DATA : \",mae)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "boston data shape (row,column) :  (506, 14)\n",
            "total_data :  506\n",
            "train_data :  303\n",
            "validation_data :  101\n",
            "test_data :  102\n",
            "=========== GRADIENT DESCENT ALGORITHM STARTS : ==============\n",
            "theta from training set :  [[ 0.02277276]\n",
            " [ 0.07891455]\n",
            " [ 0.06918869]\n",
            " [-0.01713093]\n",
            " [ 0.02599327]\n",
            " [ 0.0184991 ]\n",
            " [ 0.44055185]\n",
            " [ 0.08587467]\n",
            " [-0.15999092]\n",
            " [ 0.17182027]\n",
            " [-0.00092437]\n",
            " [-0.15714102]\n",
            " [ 0.07412098]\n",
            " [-0.85012851]]\n",
            "theta after tuning alpha from validation set :  [[ 0.04225091]\n",
            " [-0.12855066]\n",
            " [ 0.06273439]\n",
            " [-0.07674999]\n",
            " [ 0.10012007]\n",
            " [-0.00796301]\n",
            " [ 0.10373847]\n",
            " [ 0.08264848]\n",
            " [-0.23843666]\n",
            " [ 0.18461958]\n",
            " [ 0.004664  ]\n",
            " [ 0.483971  ]\n",
            " [ 0.0399667 ]\n",
            " [-1.02870376]]\n",
            "MEAN ABSOLUTE ERROR ON TRAINING DATA :  6.227073327937616\n",
            "MEAN ABSOLUTE ERROR ON VALIDATING DATA :  4.592752365421676\n",
            "MEAN ABSOLUTE ERROR ON TEST  DATA :  6.030333031725009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFLr4BwI_EOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}